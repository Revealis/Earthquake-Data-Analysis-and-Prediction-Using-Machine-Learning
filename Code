import pandas as pd
import matplotlib.pyplot as plt
import folium
from folium.plugins import HeatMap
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler


file_path = "veriler.csv"
df = pd.read_csv(file_path)


df['Olus tarihi'] = pd.to_datetime(df['Olus tarihi'], format='%Y.%m.%d')


plt.figure(figsize=(12,6))
df['Olus tarihi'].dt.year.value_counts().sort_index().plot(kind='bar', color='b')
plt.xlabel('Year')
plt.ylabel('Number of Earthquakes')
plt.title('Earthquake Count by Year')
plt.show()


m = folium.Map(location=[df['Enlem'].mean(), df['Boylam'].mean()], zoom_start=6)


bins = {
    "4-4.9": df[(df['ML'] >= 4.0) & (df['ML'] < 5.0)],
    "5-5.9": df[(df['ML'] >= 5.0) & (df['ML'] < 6.0)],
    "6-6.9": df[(df['ML'] >= 6.0) & (df['ML'] < 7.0)],
    "7+": df[df['ML'] >= 7.0]
}

for label, data in bins.items():
    heat_data = list(zip(data['Enlem'], data['Boylam'], data['ML']))
    HeatMap(heat_data, name=label).add_to(m)

folium.LayerControl().add_to(m)

m.save("earthquake_map.html")



df_ml = df[['Der(km)', 'Enlem', 'Boylam', 'ML', 'Mw', 'Ms', 'Mb']].dropna()
X = df_ml.drop(columns=['ML'])
y = df_ml['ML']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)


xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)


scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
y_scaled = scaler.fit_transform(y.values.reshape(-1, 1))


X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)
X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1))
X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1))


lstm_model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)
])

lstm_model.compile(optimizer='adam', loss='mean_squared_error')
lstm_model.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=16, verbose=1)
y_pred_lstm = lstm_model.predict(X_test_lstm)


mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))

mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))

mae_lstm = mean_absolute_error(y_test_lstm, y_pred_lstm)
rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm, y_pred_lstm))

print(f"Random Forest - MAE: {mae_rf}, RMSE: {rmse_rf}")
print(f"XGBoost - MAE: {mae_xgb}, RMSE: {rmse_xgb}")
print(f"LSTM - MAE: {mae_lstm}, RMSE: {rmse_lstm}")
